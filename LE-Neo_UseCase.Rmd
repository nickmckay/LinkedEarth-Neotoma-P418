---
title: "LinkedEarth - Neotoma - P418 Use Case"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

#Isabel & age uncertainty

Allow me to paint a picture for you. A user (let's call her Isabel) is interested in investigating the impact of time uncertainty on a pollen-based temperature reconstruction at Basin Pond. The temperature data are available at LinkedEarth, but the geochronologic data and depth data are not archived there. The pollen, depth and geochronologic data are available at Neotoma, but the temperature data are not. Isabel wants to combine these two datasets, and then use the GeoChronR package to do age-uncertain viusalization.

This is a story of how she might do that.





1. Isabel decides to search for the Basin Pond data geographically, and creates a box around the site.

```{r}
locpoly <- matrix(c(-70,-71,-71,-70,45,45,44,44),ncol=2,byrow = F)
```
2. She then converts that array to GeoJSON FeatureCollection, so she can query GeoDex

```{r}
#install.packages("geojson")
library(geojson)

poly <- geojson::polygon(paste0('{ "type": "Polygon", "coordinates": [[', "[",locpoly[1,1],", ",locpoly[1,2],"],", " [",locpoly[2,1],", ",locpoly[2,2],"],", " [",locpoly[3,1],", ",locpoly[3,2],"],", " [",locpoly[4,1],", ", locpoly[4,2],"],", " [",locpoly[1,1],", ", locpoly[1,2],']]] }'))

feature <- geojson::feature(poly)
feature_collection <- geojson::featurecollection(feature)
feature_collection_str <- geo_pretty(feature_collection)
```

3: She then sets options for spatial searching, the domain and request URL details. Please see http://geodex.org/swagger-ui/ for a complete description of the web service call formats.

```{r}
search_type <- "spatial/search/object"
domain_url <- "http://geodex.org/api/v1/"
request_url <- paste(domain_url, search_type, sep="")
```

4. She creates an R list data structure to hold URL submission parameters

```{r}
params_list <- list(geowithin = feature_collection_str)
```

5. Then makes a call to the Geodex RESTful web service using the requests package.

```{r}
library(httr)
r <- GET(request_url, query = params_list)
results <- content(r, "text", encoding = "ISO-8859-1")
```

6. OK! The first results. She takes a look at what her spatial search returned.
```{r}
library(rjson)
results_json <- fromJSON(results)
features <- results_json$features
number_of_results <- length(features)
first_result <- features[[1]]
url <- first_result$properties$URL
```

7. Since it's a spatial search, she decides to sort results by distance to polygon
```{r}

library(geosphere)
#find nearest coord in each feature
nearestInFeature <- function(feat){
  if(is.list(feat$geometry$coordinates)){
    coords <- matrix(unlist((feat$geometry$coordinates)),ncol=2,byrow = T)
  }else{
    coords <- matrix((feat$geometry$coordinates),ncol=2,byrow = T)
  }
  md =try(min(geosphere::distGeo(coords,locpoly[1,])))
  if(is.numeric(md)){
    return(md)
  }else{
      return(NA)
    }

}
nearestDist <- sapply(results_json$features,nearestInFeature)

#sort by nearest
results_json$features <- results_json$features[order(nearestDist)]
allUrls <- sapply(results_json$features,FUN = function(x){x$properties$URL})
geometry <- first_result$geometry$type
coordinates <- first_result$geometry$coordinates[[1]]

#print first 10 results
print(allUrls[1:10])
```

She recognizes those results! Numbers 2 & 7 are what she's looking for.

8. Now she explores the metadata for the LinkedEarth result to find the download link, doing another query to geodex

```{r}
dataset_url <- allUrls[7]

search_type <- "graph/details"
```
9. She creates the URL for the RESTful web service call.

```{r}
domain_url <- "http://geodex.org/api/v1/"

request_url <- paste(domain_url, search_type, sep="")
```

10. Creates and executes the GET parameter for this call.

```{r}
params_list <- list(r = dataset_url)
r <- GET(request_url, query = params_list)
results <- content(r, "text", encoding = "ISO-8859-1")
```

11. And prints the results in a useful way.

```{r}
results_json <- fromJSON(results)
print(paste("URI =", results_json$S))
print(paste("Alternate Name =", results_json$Aname))
print(paste("Name =", results_json$Name))
print(paste("URL =", results_json$URL))
print(paste("Description =", results_json$Description))
print(paste("Citation =", results_json$Citation))
print(paste("Data Published =", results_json$Datepublished))
print(paste("Dataset Download Link =", results_json$Curl))
print(paste("Keywords =", results_json$Keywords))
print(paste("License =", results_json$License))
```


12. Aha! There's the download URL she needs! She'll use the LiPD-utilities package to load that dataset into R

```{r}
#devtools::install_github("nickmckay/lipd-utilities",subdir = "R")
library(lipdR)
L2 <- readLipd(results_json$Curl)
```

13. Now to get her hands on those Neotoma data. She needs the site ID from the Neotoma metadata, and performs another GeoDex query to achieve this...

```{r}
dataset_url <- allUrls[2]
```

... creating the GET parameter for this call ...

```{r}
params_list <- list(r = dataset_url)
```

... and executing the RESTful web service call.

```{r}
r <- GET(request_url, query = params_list)
results <- content(r, "text", encoding = "ISO-8859-1")
```

14. She prints these results in a user friendly way.

```{r}
results_json <- fromJSON(results)
print(paste("URI =", results_json$S))
print(paste("Alternate Name =", results_json$Aname))
print(paste("Name =", results_json$Name))
print(paste("URL =", results_json$URL))
print(paste("Description =", results_json$Description))
print(paste("Citation =", results_json$Citation))
print(paste("Data Published =", results_json$Datepublished))
print(paste("Dataset Download Link =", results_json$Curl))
print(paste("Keywords =", results_json$Keywords))
print(paste("License =", results_json$License))
```

TBD: This search needs needs to return a site.id or a site name.

15. Now she can load in Neotoma data as a LiPD object too!

```{r}
#devtools::install_github("nickmckay/geochronr")
library(geoChronR)
library(neotoma)
L = neotoma2Lipd(site = 234)#PULL THIS FROM QUERY RESULTS!
#estimate uncertainty from range
L$chronData[[2]]$measurementTable[[1]] = estimateUncertaintyFromRange(L$chronData[[2]]$measurementTable[[1]])
detach("package:geojson",character.only = TRUE) #deal with polygon conflict!
```

16. Now Isabel uses the geochronologic data from LinkedEarth to create a new age model in Bacon.
```{r}
L = runBacon(L,which.chron = 2,baconDir = "~/Dropbox/MacBacon/",modelNum = 2,
             remove.rejected = FALSE,labIDVar="chron.control.id", age14CVar = NULL, age14CuncertaintyVar = NULL, ageVar = "age",ageUncertaintyVar = "unc.estimate", depthVar = "depth", reservoirAge14CVar = NULL,reservoirAge14CUncertaintyVar = NULL,rejectedAgesVar=NULL,useMarine = FALSE,BaconAsk = FALSE, baconAccMean = 1,BaconSuggest = FALSE)
              
```


17. And geoChronR to visualize her new age model and a handful of ensemble members
```{r}
plotChron(L,chron.number = 2,model.num = 2)
```

18. Next, she takes the temperature data from the LinkedEarth file, and ports it into the Neotoma-derived object, and maps the age ensemble onto the depths associated with the temperature values.
```{r}
L$paleoData[[1]]$measurementTable[[1]]$temperature <- L2$paleoData[[1]]$measurementTable[[1]]$temperature
L <- mapAgeEnsembleToPaleoData(L = L,which.chron = 2,which.paleo = 1,which.model = 2,which.ens = 1,which.pmt = 1,age.var = "ageEnsemble")
```

19. Finally, Isabel creates a plot that shows the impact of age uncertainty on the temperature dataset.

```{r}
library(ggplot2)
temp <- selectData(L,"temperature")
ageEnsemble <- selectData(L,"ageEnsemble")

plot <- plotTimeseriesEnsRibbons(ageEnsemble,temp)
plot <- plotTimeseriesEnsLines(ageEnsemble,temp,color = "red",add.to.plot = plot,maxPlotN = 5)+
  scale_x_reverse("Age (yr BP)")
print(plot)
```

The end.